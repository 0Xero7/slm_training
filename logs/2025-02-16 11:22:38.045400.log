
use_experimental: True
--------------------------------------------------------------------------------
max_seq_len: 128
d_model: 512
num_layers: 12
ff_hidden: 2048
lr: 0.0001
num_epochs: 30
batch_size: 128
grad_clip: 0.5
weight_decay: 0.01
--------------------------------------------------------------------------------
Model Parameter Count: 66829394
--------------------------------------------------------------------------------
Epoch: 1/30
Time Taken: 138.22s - LR: 9.98e-05
Train Loss: 7.2979 - Train Perplexity: 1477.26
Valid Loss: 6.5209 - Valid Perplexity: 679.21
--------------------------------------------------------------------------------
Epoch: 2/30
Time Taken: 30.29s - LR: 9.90e-05
Train Loss: 6.2881 - Train Perplexity: 538.14
Valid Loss: 6.1215 - Valid Perplexity: 455.54
--------------------------------------------------------------------------------
Epoch: 3/30
Time Taken: 30.30s - LR: 9.78e-05
Train Loss: 5.9364 - Train Perplexity: 378.56
Valid Loss: 5.8966 - Valid Perplexity: 363.79
--------------------------------------------------------------------------------
Epoch: 4/30
Time Taken: 30.12s - LR: 9.61e-05
Train Loss: 5.6946 - Train Perplexity: 297.27
Valid Loss: 5.7322 - Valid Perplexity: 308.64
--------------------------------------------------------------------------------
Epoch: 5/30
Time Taken: 30.36s - LR: 9.40e-05
Train Loss: 5.5080 - Train Perplexity: 246.65
Valid Loss: 5.6092 - Valid Perplexity: 272.92
--------------------------------------------------------------------------------
Epoch: 6/30
Time Taken: 30.29s - LR: 9.14e-05
Train Loss: 5.3553 - Train Perplexity: 211.72
Valid Loss: 5.5185 - Valid Perplexity: 249.26
--------------------------------------------------------------------------------
Epoch: 7/30
Time Taken: 30.20s - LR: 8.84e-05
Train Loss: 5.2255 - Train Perplexity: 185.95
Valid Loss: 5.4400 - Valid Perplexity: 230.44
--------------------------------------------------------------------------------
Epoch: 8/30
Time Taken: 30.24s - LR: 8.51e-05
Train Loss: 5.1120 - Train Perplexity: 166.00
Valid Loss: 5.3824 - Valid Perplexity: 217.54